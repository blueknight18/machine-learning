{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST via Fully Connected Deep Neural Network with 4 Hidden Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input layer (x), hidden layers, output layer\n",
    "n_inputs = 28*28 # = 784\n",
    "n_hidden1 = 400\n",
    "n_hidden2 = 200\n",
    "n_hidden3 = 100\n",
    "n_hidden4 = 50\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = (None, n_inputs), name = \"x\") #784-dimensional tensor that can be injected with data, images\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = \"y\") #batch_size vector of class ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"layers\"): #label for this part of the graph, the layers of the neural network\n",
    "    hidden1 = tf.layers.dense(x, n_hidden1, name = \"hidden1\", activation = tf.nn.relu)\n",
    "    #Creates densely connected layer of a neural network which outputs activation(input + bias)\n",
    "    #Takes x (which will be fed) as input and has n_hidden1 units\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, name = \"hidden2\", activation = tf.nn.relu)\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, name = \"hidden3\", activation = tf.nn.relu)\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, name = \"hidden4\", activation = tf.nn.relu)\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name = \"outputs\")\n",
    "    #Output layer with 10 nodes taking inputs from the fourth hidden layer\n",
    "    #Each layer returns an output tensor (array of outputs of each neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loss function (cross-entropy)\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(xentropy, name = \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.125 #Best learning rate according to my tests\n",
    "\n",
    "with tf.name_scope(\"minimize\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate) #Implements gradient descent algorithm\n",
    "    train_op = optimizer.minimize(loss) #Minimizes loss by updating variables (weights and threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1) \n",
    "    #Says whether the targets are in the top k (1) predictions\n",
    "    #tf.nn.in_top_k(predictions, targets, k)\n",
    "    #Returns a Tensor of type bool\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) \n",
    "    #tf.reduce_mean: Computes the mean of elements across dimensions of a tensor\n",
    "    #tf.cast: Casts Tensor to a new type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() #Saves model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 \tTrain accuracy 0.9800 \tTest accuracy 0.9638\n",
      "Epoch  1 \tTrain accuracy 1.0000 \tTest accuracy 0.9760\n",
      "Epoch  2 \tTrain accuracy 1.0000 \tTest accuracy 0.9678\n",
      "Epoch  3 \tTrain accuracy 1.0000 \tTest accuracy 0.9772\n",
      "Epoch  4 \tTrain accuracy 1.0000 \tTest accuracy 0.9700\n",
      "Epoch  5 \tTrain accuracy 1.0000 \tTest accuracy 0.9825\n",
      "Epoch  6 \tTrain accuracy 1.0000 \tTest accuracy 0.9803\n",
      "Epoch  7 \tTrain accuracy 1.0000 \tTest accuracy 0.9801\n",
      "Epoch  8 \tTrain accuracy 1.0000 \tTest accuracy 0.9806\n",
      "Epoch  9 \tTrain accuracy 1.0000 \tTest accuracy 0.9801\n",
      "Epoch 10 \tTrain accuracy 1.0000 \tTest accuracy 0.9804\n",
      "Epoch 11 \tTrain accuracy 1.0000 \tTest accuracy 0.9820\n",
      "Epoch 12 \tTrain accuracy 1.0000 \tTest accuracy 0.9810\n",
      "Epoch 13 \tTrain accuracy 1.0000 \tTest accuracy 0.9847\n",
      "Epoch 14 \tTrain accuracy 1.0000 \tTest accuracy 0.9855\n",
      "Epoch 15 \tTrain accuracy 1.0000 \tTest accuracy 0.9857\n",
      "Epoch 16 \tTrain accuracy 1.0000 \tTest accuracy 0.9856\n",
      "Epoch 17 \tTrain accuracy 1.0000 \tTest accuracy 0.9857\n",
      "Epoch 18 \tTrain accuracy 1.0000 \tTest accuracy 0.9856\n",
      "Epoch 19 \tTrain accuracy 1.0000 \tTest accuracy 0.9856\n",
      "\n",
      "Final Accuracy 0.9857000112533569\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    final_accuracy = 0\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for step in range(mnist.train.num_examples // batch_size): # // means integer division\n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size) #Batch of images and batch of labels\n",
    "            sess.run(train_op, feed_dict={x: x_batch, y: y_batch}) #Applying gradient descent to batch\n",
    "        acc_train = accuracy.eval(feed_dict={x: x_batch, y: y_batch}) #Accuracy score on training set\n",
    "        acc_test = accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels}) #Accuracy score on test set\n",
    "        print(\"Epoch {:>2d}\".format(epoch), \"\\tTrain accuracy {:.4f}\".format(acc_train), \n",
    "                                                  \"\\tTest accuracy {:.4f}\".format(acc_test))\n",
    "        if acc_test > final_accuracy:\n",
    "            final_accuracy = acc_test\n",
    "            save_model = saver.save(sess, \"models/my_model_final.ckpt\") #Save the model with the highest accuracy\n",
    "    print(\"\\nFinal Accuracy {}\".format(final_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9286\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d68b39e9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "len = mnist.test.images.shape[0]\n",
    "index = random.randrange(len)\n",
    "print(index)\n",
    "\n",
    "adigit = mnist.test.images[index]\n",
    "adigit_image = adigit.reshape(28,28)\n",
    "\n",
    "plt.imshow(adigit_image, cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(mnist.test.labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "adigit = adigit.reshape(1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/my_model_final.ckpt\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"models/my_model_final.ckpt\")\n",
    "    pred = tf.argmax(logits, 1)\n",
    "    print(pred.eval(feed_dict = {x: adigit}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
